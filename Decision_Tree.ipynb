{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "K854w91xNmTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decision Tree is a supervised learning algorithm commonly used for classification. It has a tree-like model where internal nodes represent decision rules based on features, branches represent outcomes, and leaf nodes represent class labels.\n",
        "\n",
        "Working: The algorithm splits data step by step using measures like Information Gain or Gini Index, until the final leaf assigns a class.\n",
        "It is easy to interpret, handles both categorical and numerical data, and shows the decision-making process clearly."
      ],
      "metadata": {
        "id": "sxnbaTP6OEaN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "Answer:"
      ],
      "metadata": {
        "id": "0iOz6GWOOtjz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Decision Trees, impurity measures decide how to split data.\n",
        "\n",
        "Gini Impurity: Measures the probability of incorrectly classifying a randomly chosen element. A value of 0 means perfect purity.\n",
        "\n",
        "Entropy: Based on information theory, it measures the level of uncertainty or disorder in data. A value of 0 means no randomness (pure).\n",
        "\n",
        "Impact on Splits: Both are used to select the best feature for splitting. The algorithm chooses the split that reduces impurity the most, creating purer child nodes and improving classification accuracy."
      ],
      "metadata": {
        "id": "W8my3FCxOysT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "Dl0NS49hPcSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-pruning stops the growth of a Decision Tree early by setting conditions such as maximum depth, minimum samples per node, or minimum information gain, to avoid overfitting. Post-pruning, on the other hand, allows the tree to grow fully and then removes branches that add little value. The practical advantage of pre-pruning is that it reduces training time and keeps the model simpler, while post-pruning usually results in better generalization since it evaluates the full tree before trimming unnecessary parts."
      ],
      "metadata": {
        "id": "rQxr_r0IPjTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "QbbDdnloPtYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Information Gain is a metric used in Decision Trees to measure how well a feature separates the data into classes. It is calculated as the reduction in entropy after a dataset is split based on a feature. A higher Information Gain means the feature provides better classification by making the child nodes purer. It is important because the algorithm selects the split with the highest Information Gain, ensuring that each step of the tree construction leads to more accurate and efficient classification."
      ],
      "metadata": {
        "id": "a1HBOa9lPUPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "nZP-At2eP2uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Trees are widely used in real-world applications such as medical diagnosis, credit risk assessment, fraud detection, customer segmentation, and recommendation systems. Their main advantages are that they are simple to understand, easy to visualize, require little data preprocessing, and can handle both categorical and numerical features. However, their limitations include a tendency to overfit, sensitivity to small changes in data, and lower accuracy compared to more advanced ensemble methods like Random Forests or Gradient Boosting."
      ],
      "metadata": {
        "id": "WNH5QfR5P9uF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset Info:\n",
        "● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "● Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier using the Gini criterion\n",
        "● Print the model’s accuracy and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:\n"
      ],
      "metadata": {
        "id": "SFxlBNViRYjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris, fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
        "\n",
        "iris = load_iris()\n",
        "X_iris, y_iris = iris.data, iris.target\n",
        "\n",
        "X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train_i, y_train_i)\n",
        "\n",
        "y_pred_i = clf.predict(X_test_i)\n",
        "print(\"Iris Dataset - Decision Tree Classifier\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_i, y_pred_i))\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Note: Boston Housing (`load_boston`) has been removed in recent scikit-learn versions due to ethical concerns.\n",
        "# California Housing is used here as a modern replacement.\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_h, y_h = housing.data, housing.target\n",
        "\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_h, y_h, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train_h, y_train_h)\n",
        "\n",
        "y_pred_h = regressor.predict(X_test_h)\n",
        "print(\"California Housing Dataset - Decision Tree Regressor\")\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test_h, y_pred_h))\n",
        "print(\"R^2 Score:\", r2_score(y_test_h, y_pred_h))\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lnwwIzLWrPj",
        "outputId": "b35786b4-ae94-4e39-ea94-692f369da825"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Dataset - Decision Tree Classifier\n",
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n",
            "\n",
            "\n",
            "California Housing Dataset - Decision Tree Regressor\n",
            "Mean Squared Error: 0.495235205629094\n",
            "R^2 Score: 0.622075845135081\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "UFSEP8aHYZ8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "y_pred_full = full_tree.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "y_pred_pruned = pruned_tree.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "\n",
        "print(\"Fully-grown tree Accuracy:\", accuracy_full)\n",
        "print(\"Max_depth=3 tree Accuracy:\", accuracy_pruned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmBVdd5bYh78",
        "outputId": "b8d85f4c-35e7-4b67-e82d-a16a84cc968b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully-grown tree Accuracy: 1.0\n",
            "Max_depth=3 tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Load the California Housing dataset from sklearn\n",
        "● Train a Decision Tree Regressor\n",
        "● Print the Mean Squared Error (MSE) and feature importances\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "ha3PdvMVY_dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"R^2 Score:\", r2_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", regressor.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iXT93JrZigY",
        "outputId": "fbd3bd6e-8041-4d2b-db01-fc4b185c9ef0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "R^2 Score: 0.622075845135081\n",
            "Feature Importances: [0.52850909 0.05188354 0.05297497 0.02866046 0.03051568 0.13083768\n",
            " 0.09371656 0.08290203]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Load the Iris Dataset\n",
        "● Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "● Print the best parameters and the resulting model accuracy\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "0yUfYtfIZ1ZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'max_depth': [1, 2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Model Accuracy with Best Parameters:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOKNBrZtZ7rN",
        "outputId": "4fdc5361-a427-43bc-9145-24018a54326a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Model Accuracy with Best Parameters: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "● Handle the missing values\n",
        "● Encode the categorical features\n",
        "● Train a Decision Tree model\n",
        "● Tune its hyperparameters\n",
        "● Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "lRrMV5TKaQZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a Decision Tree model for predicting disease in a healthcare dataset with mixed data types and missing values, the following steps can be taken:\n",
        "\n",
        "Handle Missing Values: First, identify missing data in the dataset. For numerical features, missing values can be imputed using mean or median values, while categorical features can use mode or a separate category for missing data. Advanced techniques like K-Nearest Neighbors imputation can also be applied.\n",
        "\n",
        "Encode Categorical Features: Convert categorical variables into numerical form using techniques like one-hot encoding or label encoding so that the Decision Tree can process them.\n",
        "\n",
        "Train the Decision Tree Model: Split the dataset into training and testing sets and train a Decision Tree Classifier. Start with default hyperparameters to establish a baseline performance.\n",
        "\n",
        "Hyperparameter Tuning: Use techniques like GridSearchCV or RandomizedSearchCV to tune parameters such as max_depth, min_samples_split, and criterion to improve model accuracy and prevent overfitting.\n",
        "\n",
        "Evaluate Performance: Evaluate the model using metrics such as accuracy, precision, recall, F1-score, and ROC-AUC, depending on the business focus (e.g., minimizing false negatives in healthcare). Cross-validation can provide more reliable performance estimates.\n",
        "\n",
        "Business Value: This model can help healthcare providers identify high-risk patients early, enabling timely interventions and treatment. It can also assist in resource allocation, reduce diagnostic costs, and improve patient outcomes by predicting disease presence with high accuracy."
      ],
      "metadata": {
        "id": "VtNlRUN4ac07"
      }
    }
  ]
}